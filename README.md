# Minimal-Hand
Minimal-Hand based Pytorch (CVPR2020)
This is a **unofficial** implementation of Minimal-Hand based on PyTorch  


This project provides the core components for hand motion capture:

- estimating joint locations from a monocular RGB image (DetNet)
- estimating joint rotations from locations (IKNet)

## DO
- [x] Inference code like [offical one](https://github.com/CalciferZh/minimal-hand)
- [ ] Training code

# Citation
This is the **unofficial** implementation of the paper "Monocular Real-time Hand Shape and Motion Capture using Multi-modal Data" (CVPR 2020).


If you find the project helpful, please consider citing them:
```
@inproceedings{zhou2020monocular,
  title={Monocular Real-time Hand Shape and Motion Capture using Multi-modal Data},
  author={Zhou, Yuxiao and Habermann, Marc and Xu, Weipeng and Habibie, Ikhsanul and Theobalt, Christian and Xu, Feng},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={5346--5355},
  year={2020}
}
```

# Differents between offical code and us
[Link](Different/Different.md)
